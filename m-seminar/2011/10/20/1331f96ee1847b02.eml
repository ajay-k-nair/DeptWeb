Delivered-To: siddhartha.gadgil@gmail.com
Received: by 10.231.15.76 with SMTP id j12cs161446iba;
        Wed, 19 Oct 2011 21:29:05 -0700 (PDT)
Received: by 10.68.30.65 with SMTP id q1mr17464462pbh.91.1319084945386;
        Wed, 19 Oct 2011 21:29:05 -0700 (PDT)
Return-Path: <bharali@math.iisc.ernet.in>
Received: from relay.iisc.ernet.in (relay.iisc.ernet.in. [203.200.35.70])
        by mx.google.com with ESMTPS id d6si4794647pbw.161.2011.10.19.21.29.04
        (version=TLSv1/SSLv3 cipher=OTHER);
        Wed, 19 Oct 2011 21:29:05 -0700 (PDT)
Received-SPF: pass (google.com: domain of bharali@math.iisc.ernet.in designates 203.200.35.70 as permitted sender) client-ip=203.200.35.70;
Authentication-Results: mx.google.com; spf=pass (google.com: domain of bharali@math.iisc.ernet.in designates 203.200.35.70 as permitted sender) smtp.mail=bharali@math.iisc.ernet.in
Received: from math.iisc.ernet.in (math.iisc.ernet.in [10.134.1.11])
	by relay.iisc.ernet.in (8.13.1/8.13.1) with ESMTP id p9K4RcO3026860;
	Thu, 20 Oct 2011 09:57:38 +0530
Received: from math.iisc.ernet.in (localhost.localdomain [127.0.0.1])
	by math.iisc.ernet.in (8.13.8/8.13.8) with ESMTP id p9K4XDTf016772
	for <m_seminar@math.iisc.ernet.in>; Thu, 20 Oct 2011 10:03:13 +0530
Received: from localhost (bharali@localhost)
	by math.iisc.ernet.in (8.13.8/8.13.8/Submit) with ESMTP id p9K4XDrU016769
	for <m_seminar@math.iisc.ernet.in>; Thu, 20 Oct 2011 10:03:13 +0530
Date: Thu, 20 Oct 2011 10:03:13 +0530 (IST)
From: Gautam Bharali <bharali@math.iisc.ernet.in>
To: m_seminar@math.iisc.ernet.in
Subject: [Seminar] Math: Today: "Challenges in high dimensional Bayesian
 nonparametrics and some possible solutions"
Message-ID: <Pine.LNX.4.64.1110201001520.16626@math.iisc.ernet.in>
MIME-Version: 1.0
Content-Type: MULTIPART/Mixed; BOUNDARY="-2046162175-125986894-1318481538=:27207"
Content-ID: <Pine.LNX.4.64.1110200959461.16626@math.iisc.ernet.in>
X-IISc-MailScanner-Information: Please contact the ISP for more information
X-IISc-MailScanner: Found to be clean
X-IISc-MailScanner-SpamCheck: not spam, SpamAssassin (not cached, score=-2.9,
	required 6.5, autolearn=not spam, ALL_TRUSTED -1.00, BAYES_00 -1.90)
X-IISc-MailScanner-From: bharali@math.iisc.ernet.in
X-Spam-Status: No

  This message is in MIME format.  The first part should be readable text,
  while the remaining parts are likely unreadable without MIME-aware tools.

---2046162175-125986894-1318481538=:27207
Content-Type: TEXT/PLAIN; CHARSET=ISO-8859-1; FORMAT=flowed
Content-Transfer-Encoding: 8BIT
Content-ID: <Pine.LNX.4.64.1110200959462.16626@math.iisc.ernet.in>


  			Department of Mathematics
  		   Indian Institute of Science, Bangalore


  				SEMINAR


Speaker	: Prof. Anjishnu Banerjee
 	  Duke University, USA

Title	: Challenges in high dimensional Bayesian
 	  nonparametrics and some possible solutions

Date	: October 20, 2011

Time	: 11:30 a.m.-12:30 p.m.

Venue	: Lecture Hall III, Department of Mathematics


**PLEASE NOTE THE UNUSUAL TIME AND VENUE**


Abstract:
Large dimensional data presents many challenges for
statistical modeling via Bayesian nonparametrics, both with respect
to theroetical issues and computational aspects. We discuss some
models that can accomodate large dimensional data and have attractive
theoretical properties, specially focussing on kernel partition
processes, which are a generalization of the well known Dirichlet
Processes. We discuss issues of consistency. We then move onto some
typical computational problems in Bayesian nonparametrics, focussing
initially on Gaussian processes (GPs). GPs are widely used in
nonparametric regression, classification and spatio-temporal
modeling, motivated in part by a rich literature on theoretical
properties.  However, a well known drawback of GPs that limits their
use is the expensive computation, typically O($n^3$) in performing
the necessary matrix inversions with $n$ denoting the number of data
points.  In large data sets, data storage and processing also lead to
computational bottlenecks and numerical stability of the estimates
and predicted values degrades with $n$.  To address these problems, a
rich variety of methods have been proposed, with recent options
including predictive processes in spatial data analysis and subset of
regressors in machine learning.  The underlying idea in these
approaches is to use a subset of the data, leading to questions of
sensitivity to the subset and limitations in estimating fine scale
structure in regions that are not well covered by the subset. 
Partially motivated by the literature on compressive sensing, we
propose an alternative random projection of all the data points onto
a lower-dimensional subspace, which also allows for easy
parallelizability for further speeding computation.  We connect this
with a wide class of matrix approximation techniques. We demonstrate
the superiority of this approach from a theoretical perspective and
through the use of simulated and real data examples. We finally
consider extensions of these approaches for dimension reduction in
other non parametric models.


  		    ALL ARE CORDIALLY INVITED

  						Chairman,
  						Department of Mathematics
-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.


---2046162175-125986894-1318481538=:27207--
