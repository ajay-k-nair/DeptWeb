Delivered-To: siddhartha.gadgil@gmail.com
Received: by 10.231.15.76 with SMTP id j12cs154748iba;
        Wed, 12 Oct 2011 21:49:20 -0700 (PDT)
Received: by 10.68.26.101 with SMTP id k5mr6144205pbg.4.1318481359995;
        Wed, 12 Oct 2011 21:49:19 -0700 (PDT)
Return-Path: <bharali@math.iisc.ernet.in>
Received: from relay.iisc.ernet.in (relay.iisc.ernet.in. [203.200.35.65])
        by mx.google.com with ESMTPS id p3si2608729pbd.164.2011.10.12.21.49.18
        (version=TLSv1/SSLv3 cipher=OTHER);
        Wed, 12 Oct 2011 21:49:19 -0700 (PDT)
Received-SPF: pass (google.com: domain of bharali@math.iisc.ernet.in designates 203.200.35.65 as permitted sender) client-ip=203.200.35.65;
Authentication-Results: mx.google.com; spf=pass (google.com: domain of bharali@math.iisc.ernet.in designates 203.200.35.65 as permitted sender) smtp.mail=bharali@math.iisc.ernet.in
Received: from math.iisc.ernet.in (math.iisc.ernet.in [10.134.1.11])
	by relay.iisc.ernet.in (8.14.4/8.14.4) with ESMTP id p9D4oMOB011574;
	Thu, 13 Oct 2011 10:20:22 +0530
Received: from math.iisc.ernet.in (localhost.localdomain [127.0.0.1])
	by math.iisc.ernet.in (8.13.8/8.13.8) with ESMTP id p9D4r5Qg027549
	for <m_seminar@math.iisc.ernet.in>; Thu, 13 Oct 2011 10:23:05 +0530
Received: from localhost (bharali@localhost)
	by math.iisc.ernet.in (8.13.8/8.13.8/Submit) with ESMTP id p9D4r5nU027546
	for <m_seminar@math.iisc.ernet.in>; Thu, 13 Oct 2011 10:23:05 +0530
Date: Thu, 13 Oct 2011 10:23:05 +0530 (IST)
From: Gautam Bharali <bharali@math.iisc.ernet.in>
To: m_seminar@math.iisc.ernet.in
Subject: [Seminar] Math: 20th Oct: "Challenges in high dimensional Bayesian
 nonparametrics and some possible solutions"
Message-ID: <Pine.LNX.4.64.1110131022230.27207@math.iisc.ernet.in>
MIME-Version: 1.0
Content-Type: MULTIPART/Mixed; BOUNDARY="-2046162175-125986894-1318481538=:27207"
Content-ID: <Pine.LNX.4.64.1110131022231.27207@math.iisc.ernet.in>
X-IISc-MailScanner-Information: Please contact the ISP for more information
X-MailScanner-ID: p9D4oMOB011574
X-IISc-MailScanner: Found to be clean
X-IISc-MailScanner-SpamCheck: not spam, SpamAssassin (not cached, score=-1.9,
	required 6.5, autolearn=not spam, BAYES_00 -1.90)
X-IISc-MailScanner-From: bharali@math.iisc.ernet.in
X-Spam-Status: No

  This message is in MIME format.  The first part should be readable text,
  while the remaining parts are likely unreadable without MIME-aware tools.

---2046162175-125986894-1318481538=:27207
Content-Type: TEXT/PLAIN; CHARSET=ISO-8859-1; FORMAT=flowed
Content-Transfer-Encoding: 8BIT
Content-ID: <Pine.LNX.4.64.1110131022232.27207@math.iisc.ernet.in>


  		       	  Department of Mathematics
  		    Indian Institute of Science, Bangalore


  				SEMINAR


Speaker	: Prof. Anjishnu Banerjee
 	  Duke University, USA

Title	: Challenges in high dimensional Bayesian
 	  nonparametrics and some possible solutions

Date	: October 20, 2011

Time	: 11:30 a.m.-12:30 p.m.

Venue	: Lecture Hall III, Department of Mathematics

Abstract:
Large dimensional data presents many challenges for
statistical modeling via Bayesian nonparametrics, both with respect
to theroetical issues and computational aspects. We discuss some
models that can accomodate large dimensional data and have attractive
theoretical properties, specially focussing on kernel partition
processes, which are a generalization of the well known Dirichlet
Processes. We discuss issues of consistency. We then move onto some
typical computational problems in Bayesian nonparametrics, focussing
initially on Gaussian processes (GPs). GPs are widely used in
nonparametric regression, classification and spatio-temporal
modeling, motivated in part by a rich literature on theoretical
properties.  However, a well known drawback of GPs that limits their
use is the expensive computation, typically O($n^3$) in performing
the necessary matrix inversions with $n$ denoting the number of data
points.  In large data sets, data storage and processing also lead to
computational bottlenecks and numerical stability of the estimates
and predicted values degrades with $n$.  To address these problems, a
rich variety of methods have been proposed, with recent options
including predictive processes in spatial data analysis and subset of
regressors in machine learning.  The underlying idea in these
approaches is to use a subset of the data, leading to questions of
sensitivity to the subset and limitations in estimating fine scale
structure in regions that are not well covered by the subset. 
Partially motivated by the literature on compressive sensing, we
propose an alternative random projection of all the data points onto
a lower-dimensional subspace, which also allows for easy
parallelizability for further speeding computation.  We connect this
with a wide class of matrix approximation techniques. We demonstrate
the superiority of this approach from a theoretical perspective and
through the use of simulated and real data examples. We finally
consider extensions of these approaches for dimension reduction in
other non parametric models.


  		    ALL ARE CORDIALLY INVITED

  					      Chairman,
  					      Department of Mathematics
-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.


---2046162175-125986894-1318481538=:27207--
